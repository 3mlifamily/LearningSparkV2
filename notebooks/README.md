## Running Notebooks

We have provided Python and Scala notebooks in the LearningSparkv2.dbc file. Follow the steps below to import these notebooks into Databricks. Most of this code should run outside the Databricks environment.

* Step 1: Register for a free [Databricks Community Edition](https://www.databricks.com/try-databricks) account
* Step 2: Login
* Step 3: Create a cluster with Spark 3.0 and Databricks Runtime 7.x ML. The ML runtime pre-installs many of the common ML libraries, some of which are used in chapter 11 (e.g. MLflow, XGBoost, etc.)
* Step 4: [import a notebook](https://docs.databricks.com/notebooks/notebooks-manage.html#import-a-notebook).

Once you import these notebooks into Databricks, you can export them as an .ipynb and run them in a Jupyter notebook. To run Spark in a Jupyter notebook, follow these [instructions]().
